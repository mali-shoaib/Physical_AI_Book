# Implementation Plan: Embedding Pipeline Setup

**Branch**: `007-embedding-pipeline` | **Date**: 2025-12-23 | **Spec**: [spec.md](spec.md)
**Input**: Feature specification from `/specs/007-embedding-pipeline/spec.md`

**Note**: This plan was generated by `/sp.plan` command following the Spec-Driven Development workflow.

## Summary

Build a complete embedding pipeline that crawls the deployed Docusaurus textbook (https://physical-ai-robotics-textbook-xi.vercel.app/), extracts and cleans text content, chunks it semantically, generates Cohere embeddings, and stores vectors with metadata in Qdrant for RAG-based retrieval. Implementation as single-file Python script (`backend/main.py`) using UV package manager for fast dependency management.

## Technical Context

**Language/Version**: Python 3.10+
**Package Manager**: UV (Astral's fast Python package manager)
**Primary Dependencies**:
- Cohere SDK (>=5.0.0) - Embedding generation
- Qdrant Client (>=1.7.0) - Vector storage
- BeautifulSoup4 (>=4.12.0) - HTML parsing
- tiktoken (>=0.5.0) - Token counting
- tenacity (>=8.2.0) - Retry logic
- loguru (>=0.7.0) - Structured logging
- python-dotenv (>=1.0.0) - Environment config

**Storage**: Qdrant Cloud (vector database) - no local storage needed
**Testing**: pytest (unit/integration tests for pipeline functions)
**Target Platform**: Cross-platform (Linux/macOS/Windows) - CLI script execution
**Project Type**: Single-file backend script (backend/main.py)
**Performance Goals**:
- Full textbook ingestion: < 30 minutes (spec: SC-006)
- Embedding generation: < 5s/chunk p95 (spec: SC-003)
- Incremental updates: < 2 minutes/chapter (spec: SC-005)
- Batch throughput: 10+ chapters/minute (research.md)

**Constraints**:
- Cohere API rate limits: Handle gracefully with exponential backoff
- Chunk size: 512-1024 tokens (spec: FR-004)
- Chunk overlap: 10-20% (spec: FR-004)
- Similarity threshold: >= 0.70 for validation (research.md)
- Zero data loss (spec: SC-007)

**Scale/Scope**:
- ~120 Docusaurus pages (production site)
- ~850 text chunks (5-10 chunks/page average)
- ~1024-dimensional vectors (Cohere embed-english-v3.0)
- Single Qdrant collection: "rag_embedding"

## Constitution Check

*GATE: Must pass before Phase 0 research. Re-check after Phase 1 design.*

### Principle I: Grounded Answers Only ✅ PASS (N/A for ingestion)

**Status**: Not applicable - this is the ingestion pipeline, not query/answer generation
**Alignment**: Pipeline stores content that will enable grounded answers in downstream RAG agent

### Principle II: Cohere Embeddings Standard ✅ PASS

**Status**: COMPLIANT
**Implementation**:
- Using `cohere>=5.0.0` SDK exclusively (research.md Decision 2)
- Model pinned: `embed-english-v3.0` with version logging
- Input type specified: `search_document` for ingestion
- No mixing of embedding providers

**Verification**: See `contracts/pipeline-functions.yaml` - `embed()` function

### Principle III: Qdrant Vector Storage ✅ PASS

**Status**: COMPLIANT
**Implementation**:
- Qdrant Client (>=1.7.0) as sole vector store
- Collection name: `rag_embedding` (fixed, per research.md)
- Cosine distance metric for similarity search
- Metadata preserved: source_url, chapter_id, module_name, heading_hierarchy

**Verification**: See `data-model.md` - Qdrant Collection entity

### Principle IV: Content Extraction Pipeline ✅ PASS

**Status**: COMPLIANT
**Implementation**: Crawl → Clean → Chunk → Embed → Store
1. **Crawl**: Sitemap XML parsing, URL filtering (`get_all_urls`)
2. **Clean**: BeautifulSoup HTML extraction, remove nav/footer (`extract_text_from_url`)
3. **Chunk**: Recursive semantic splitting, 512-1024 tokens, 10-20% overlap (`chunk_text`)
4. **Embed**: Cohere API batch calls (`embed`)
5. **Store**: Qdrant upsert with metadata (`save_chunk_to_qdrant`)

**Verification**: See `contracts/pipeline-functions.yaml` - execution_flow

### Principle V: Agent-Based Retrieval ⚠️ DEFERRED

**Status**: Out of scope for this feature (embedding pipeline only)
**Alignment**: This pipeline produces data for future RAG agent component
**Note**: Query-time retrieval will be implemented in separate feature

### Principle VI: Docusaurus Chatbot Integration ⚠️ DEFERRED

**Status**: Out of scope for this feature (backend ingestion only)
**Alignment**: Embedded content ready for chatbot integration
**Note**: Frontend chatbot will be implemented in separate feature

### Constitution Compliance Summary

| Principle | Status | Notes |
|-----------|--------|-------|
| I. Grounded Answers | N/A | Ingestion pipeline only |
| II. Cohere Embeddings | ✅ PASS | Model pinned, exclusive use |
| III. Qdrant Storage | ✅ PASS | Sole vector store |
| IV. Content Pipeline | ✅ PASS | All stages implemented |
| V. Agent Retrieval | ⚠️ DEFERRED | Future feature |
| VI. Chatbot Integration | ⚠️ DEFERRED | Future feature |

**Overall**: ✅ COMPLIANT for embedding pipeline scope

## Project Structure

### Documentation (this feature)

```text
specs/007-embedding-pipeline/
├── plan.md              # This file (implementation plan)
├── research.md          # Technology decisions and best practices
├── data-model.md        # Entity definitions and relationships
├── quickstart.md        # Setup and execution guide
├── contracts/           # Function signatures and API contracts
│   └── pipeline-functions.yaml
├── checklists/          # Quality validation checklist
│   └── requirements.md
└── tasks.md             # Will be created by /sp.tasks command
```

### Source Code (repository root)

```text
backend/                 # Embedding pipeline (this feature)
├── main.py              # Single-file implementation (all functions)
├── pyproject.toml       # UV dependencies
├── .env                 # Environment variables (git-ignored)
├── .gitignore           # Ignore .env, __pycache__, logs
├── README.md            # Pipeline documentation
└── logs/                # Execution logs (optional, git-ignored)

tests/                   # Test suite (future)
├── test_crawling.py     # URL discovery tests
├── test_extraction.py   # HTML parsing tests
├── test_chunking.py     # Semantic splitting tests
├── test_embedding.py    # Cohere API integration tests
└── test_storage.py      # Qdrant upsert tests
```

**Structure Decision**: Single-file backend script for MVP simplicity. User explicitly requested all functionality in `backend/main.py` with functional decomposition:
- `get_all_urls()` - Sitemap crawling
- `extract_text_from_url()` - HTML extraction and cleaning
- `chunk_text()` - Semantic text chunking
- `embed()` - Cohere embedding generation
- `create_collection()` - Qdrant collection initialization
- `save_chunk_to_qdrant()` - Vector storage with metadata
- `main()` - Pipeline orchestration and error handling

This structure prioritizes rapid deployment and debugging over modularity. Future refactoring can split into modules if needed.

## Complexity Tracking

> **Fill ONLY if Constitution Check has violations that must be justified**

No constitution violations. This feature is fully compliant with all applicable RAG system principles.

---

## Artifacts Generated

### Phase 0: Research ✅ COMPLETE

**File**: [research.md](research.md) (3,500+ words)

**Decisions Made** (10 total):
1. Python 3.10+ with UV package manager
2. Cohere `embed-english-v3.0` model
3. Qdrant Cloud with Python SDK
4. Requests + BeautifulSoup4 for web scraping
5. Recursive character splitting with semantic boundaries
6. Exponential backoff retry with `tenacity`
7. Metadata schema with 9 fields
8. Single-file `main.py` architecture
9. `.env` configuration with `python-dotenv`
10. Test query validation strategy

**All NEEDS CLARIFICATION items resolved**: Yes

### Phase 1: Design ✅ COMPLETE

**Files Generated**:
1. [data-model.md](data-model.md) - 6 entities with relationships, validation rules, state transitions
2. [contracts/pipeline-functions.yaml](contracts/pipeline-functions.yaml) - 7 function contracts with inputs/outputs/errors
3. [quickstart.md](quickstart.md) - Setup and execution guide with troubleshooting

**Key Design Decisions**:
- **Entities**: Web Page, Text Chunk, Embedding Vector, Qdrant Point, Qdrant Collection, Ingestion Job
- **API Contracts**: YAML spec for all pipeline functions
- **Error Handling**: 3 strategies (exponential backoff, skip-and-log, fail-fast)
- **Metadata Schema**: 9 fields per chunk for context preservation
- **Validation**: 10-20 test queries with expected results

---

## Implementation Readiness

### Ready for /sp.tasks ✅

All prerequisites met:
- ✅ Research complete (all unknowns resolved)
- ✅ Data model defined (entities + relationships)
- ✅ Contracts specified (function signatures)
- ✅ Quickstart documented (setup instructions)
- ✅ Constitution compliance verified

### Next Steps

1. Run `/sp.tasks` to generate actionable task list from plan
2. Implement tasks in priority order (P1 first)
3. Test each function independently before integration
4. Run full pipeline against production Docusaurus site
5. Validate with 50+ test queries (target: 95% success)

---

## Summary

This plan provides complete technical architecture for embedding pipeline implementation. Single-file Python script (`backend/main.py`) orchestrates 7 core functions following constitutional requirements:

- **Cohere-only** embeddings (v3.0, pinned)
- **Qdrant-only** vector storage (cosine similarity)
- **Pipeline stages**: Crawl → Clean → Chunk (512-1024 tokens) → Embed → Store
- **Target**: < 30min full ingestion, 95%+ success rate, zero data loss

All design artifacts complete and ready for task generation.
